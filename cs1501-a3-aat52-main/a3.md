This project compared four different LZW variation programs. This type of compression algorithm is most efficient when used on homogenous or repetitive data, since codewords are matched and replaced as they are added. 

The original Sedgewick code was tested first. While it worked for most of the files, its performance decreased steadily as larger files were used for testing. In some cases I had to stop the compression manually. This may be the result of the substring() that generates a new string when the method is called. This is further supported by the compression ratios getting worse as the files increase in size. Although compression took several minutes sometimes, decompression was fairly quick. The ratios indicated that code2.txt seemed to perform the best and edit.exe performed the worst. This adheres to the idea that LZW compression is best with repetitive data; human writing often repeats. The exe file was more random and the implementation may have ran out of codewords and ended up expanding the file. This LZW implementation performed the worst. 

Next, the LZWmod was tested without reset. This was more efficient and produced compression ratios that were often faster due to the mutable StringBuilder objects. The edit.exe file also performed the worst, and bmp.tar performed the best. This implementation performed the best compared to the others.

Next, the LZWmod was tested with resetting the codebook. However, any changes were only noticeable on the largest files and it made no difference with smaller files where the byte sizes were similar to those without the reset. Files frosty.jpg and large.txt had slightly bigger ratios than without codebook resetting which may have resulted from data similarity after the reset. Overall, this version performed similarly to the non-reset version, but with an average compression ratio of 41.2%, the 43% for the non-reset version was better. Similar to the test without reset, edit.exe had the worst ratio while bmp.tar performed the best. This implementation was second of the four. Reset is more efficient if it looks different in different parts so certain words become "outdated" so to speak as patterns evolve. For example a picture of a sunset would need yellow for some parts and blue for others but if something is homogenous throughout you just end up wasting space rebuilding the dict.

Finally, the Unix compress was tested. In some cases, it was able to produce much better results than the other implementations since it uses intelligent compression ratio monitoring and does not produce a compressed file if it results in an expansion. Similar to the others, edit.exe had the worst ratio while bmp.tar had the best. This implementation performed similarly efficiently to the LZWmod without reset.  

Wacky.bmp was the best file with a compression of 0.4-0.5% of the original, since the file is fairly homogenous and very long codewords can be replaced with very few bytes. Frosty.jpg and Lego-big.gif were the worst files since they were for the most part not repetitive and very noisy. This file was actually expanded by using LZW compression. 

I have also submitted a table.png that encompasses this data; take a look for specific numbers rather than a general comparison.
 


